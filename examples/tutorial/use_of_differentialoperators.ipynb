{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('bosch': conda)"
  },
  "interpreter": {
   "hash": "5471d372aecbf763f050aece93ed861d8318175083d9494b12a3b32f7831ffe8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Differentialoperators\n",
    "\n",
    "To learn a solution of a differential equation, one needs to compute different derivatives of the neural network. \n",
    "\n",
    "To make the implementation of a given ODE/PDE easier, different operators are already implemented. They can be found under\n",
    "torchphysics.utils.differentialoperators.\n",
    "\n",
    "#### Implemented Operators:\n",
    "* For scalar outputs: \n",
    "    * _grad_, to compute the gradient $\\nabla u$\n",
    "    * _laplacian_, to compute the laplace operator $\\Delta u$ \n",
    "    * _partial_, to compute a partial derivative $\\partial_x u$ \n",
    "    * _normal_derivatives_, to compute the normal derivative $\\vec{n} \\cdot \\nabla u$\n",
    "* For vector outputs:\n",
    "    * _div_, to compute the divergence $\\text{div}(u)$ or $\\nabla \\cdot u$\n",
    "    * _rot_, to compute the rotation/curl of a vector field $\\nabla \\times u$\n",
    "    * _jac_, to compute the jacobian matrix \n",
    "\n",
    "Of course, the operators for scalar outputs can also be used for vectorial outputs, if one output entry is specified. E.g. $u: D \\to \\mathbb{R}^3$ then do $\\text{laplacian}(u[:, 1], x)$, to get the laplacian of the first entry.\n",
    "\n",
    "All operators can handle the computation on a whole data batch.\n",
    "\n",
    "To better understand the usage of these operators, it follows a small example. Under the hood, all operators use the autograd functionallity of PyTorch. Therefore we need to work with tensor. To test the operators, we use the function $f: \\mathbb{R}^2 \\times \\mathbb{R} \\to \\mathbb{R}, f(x, t) = \\sin(t  x_1) + x_2^2$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Define the function:\n",
    "def f(x, t):\n",
    "    return torch.sin(t * x[:, :1]) + x[:, 1:]**2\n",
    "\n",
    "# Define some points where to evaluate the function\n",
    "x = torch.tensor([[1.0, 1.0], [0, 1], [1, 0]], requires_grad=True) \n",
    "t = torch.tensor([[1], [0], [2.0]], requires_grad=True)\n",
    "# requires_grad=True is needed, so PyTorch knows to create a backwards graph.\n",
    "# These tensors could be seen as a batch with three data points."
   ]
  },
  {
   "source": [
    "Another important part of the implemented operators is, that the input is not a function f, but the already computed outputs f(x, t). The \n",
    "advantage is, that when many different operators are used, the forward pass only happens once.\n",
    "If a custom operator is implemented, the operator also needs to work like this.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Therefore comput now the outputs:\n",
    "out = f(x, t)"
   ]
  },
  {
   "source": [
    "Let's compute the gradient and laplacian of our function, w.r.t. $x$. Analytically they are:\n",
    "$$\n",
    "    \\nabla f(x, t) = (t\\cos(tx_1), 2x_2)\n",
    "$$\n",
    "$$\n",
    "    \\Delta f(x, t) = -t^2\\sin(tx_1) + 2\n",
    "$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gradient w.r.t. x:\ntensor([[ 0.5403,  2.0000],\n        [ 0.0000,  2.0000],\n        [-0.8323,  0.0000]], grad_fn=<AddBackward0>)\nLaplace w.r.t. x:\ntensor([[ 1.1585],\n        [ 2.0000],\n        [-1.6372]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torchphysics.utils import grad, laplacian\n",
    "print('Gradient w.r.t. x:')\n",
    "print(grad(out, x))\n",
    "print('Laplace w.r.t. x:')\n",
    "print(laplacian(out, x))"
   ]
  },
  {
   "source": [
    "One could now check that these values are correct, by inputting the $x$ and $t$ values in the analytic expression.\n",
    "\n",
    "It is also possible to create the gradient/laplacian with respect to $t$. Since $t$ is one dimensional this, would be just the first/second derivative: \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gradient w.r.t. t:\ntensor([[ 0.5403],\n        [ 0.0000],\n        [-0.4161]], grad_fn=<MulBackward0>)\nLaplace w.r.t. t:\ntensor([[-0.8415],\n        [ 0.0000],\n        [-0.9093]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('Gradient w.r.t. t:')\n",
    "print(grad(out, t))\n",
    "print('Laplace w.r.t. t:')\n",
    "print(laplacian(out, t))"
   ]
  },
  {
   "source": [
    "These are the basic on how to use the implemented differential operators. The next step would be creating a condition that the neural network has to fulfill. This is shown in _create_condition.ipynb_."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}